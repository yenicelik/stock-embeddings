\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{float}
\usepackage{svg}
\usepackage{hyperref}
\usepackage{placeins}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Deep Learning for Stock Price-Movement Prediction using Stock Embeddings\\
{}
}
\author{\IEEEauthorblockN{Thomas Greminger}
\IEEEauthorblockA{\textit{ETH Zurich} \\
grethoma@student.ethz.ch}
\and
\IEEEauthorblockN{David Yenicelik*}
\IEEEauthorblockA{\textit{ETH Zurich} \\
yedavid@ethz.ch}\thanks{*Corresponding author} \\
\and
\IEEEauthorblockN{Lukas Ryll}
\IEEEauthorblockA{\textit{University of Cambridge}\\
lr487@cam.ac.uk}
\and
\IEEEauthorblockN{Konstantinos Bardis}
\IEEEauthorblockA{\textit{ETH Zurich}\\
bardis@lav.mavt.ethz.ch}\\
\and
\IEEEauthorblockN{Miller Mendoza}
\IEEEauthorblockA{\textit{ETH Zurich}\\
mmendoza@ethz.ch}

}
\maketitle

\begin{abstract}
The prediction of financial time series is an increasingly difficult task in times of big data which requires the use of efficient prediction and automation methodologies.
Traditional mathematical stochastic process models and simple machine learning models prevail in this community.
This paper demonstrates the competitiveness of deep neural network models compared to traditional machine learning models in the area of stock price prediction. \\

Our novel contribution lies in a neural architecture model through the use of stock-embeddings which are trained using no natural language input, which achieves state-of-the-art accuracy in the task of binary stock price prediction. This, also paves the way for future research in the direction of combining deep learning models with embeddings methods, in domains different than natural language processing.
\end{abstract}

\begin{IEEEkeywords}
stock embeddings, correlation measures, deep learning, machine learning
\end{IEEEkeywords}

\section{Introduction}

Artificial neural networks are becoming increasingly common in the financial markets thanks to their ability to handle large amounts of data that show non-linear behaviour, include discontinuities, and high-frequency polynomial components. Different approaches have been presented in the past, including but not limited to the use of: multi-layer feed forward neural network trained with back-propagation \cite{martinez,dhar,lasfer}, support vector machines \cite{xie,bao} and hybrid methods \cite{patel,zhu}.

Up until now the application of deep learning in financial markets is less established \cite{cavalcante}. Predicting complex, high-dimensional and noisy time series data remains a challenging task. Some previous works in this area that are worth mentioning include  \cite{yoshihara,ding,kuremoto}.

Besides the topic of using embeddings to understand correlations between stock embeddings, similar ideas for feature have been expressed by exploiting the observation that the probability density of stock market returns often resembles a power-law distribution \cite{Gopikrishnan}. This property is also found in the distribution of magnitudes of earthquakes and solar flares \cite{lippiello, mendoza}. This similarity opens up the possibility that the statistical tools used to predict these physical phenomena can be applied to predict stock prices. In this study, we explore this methodology and use it in the context of neural networks.

\section{Data}

\begin{figure}[H]
  \caption{A sample of the ABB Stock value in \$ at each timestep (day).}
  \centering
    \includegraphics[width=0.5\textwidth]{dl_ABB_sample_.png}
\end{figure}

The following dataset from Kaggle was used for the project: \textcolor{blue}{\url{https://www.kaggle.com/borismarjanovic/price-volume-data-for-all-us-stocks-etfs}}. It contains the historical daily price and volume data for 7195 US-based stocks between 1/1962 and 1/2017, including the open, high, low and close values. The data has been adjusted for dividends and splits.
Applying an 80\%-10\%-10\% train-validation-test split, there are in total 12'023'501 samples in the training set, and 1'335'945 samples in the validation and test set respectively.
A sample of the "open" prices can be seen in Fig. 1.

The following data is extracted from the aforementioned variables: 1) return one day ahead, $r_{i+1} = \frac{o_{i+1} - o_{i}}{o_i}$; 2)  return of previous day, $ r_{i-1} = \frac{o_{i} - o_{i-1}}{o_{i-1}}$; 3) return of previous two days, $r_{i-2} = \frac{o_{i} - o_{i-2}}{o_{i-1}}$; 4) return of previous five days, $r_{i-5} = \frac{o_{i} - o_{i-5}}{o_{i-5}}$, where $r$ is the return and $o_{i}$ the opening value on the $i$-th day, accordingly, as can be seen in Fig. 3. These extracted variables are appended to the data table. Moreover, the stock symbols and dates are both encoded into integer values. Finally, all NaN values are removed from the data by dropping all rows which include NaNs.

The first generated indicator is the projected return implied by the following day’s opening price. Its sign indicates whether the stock will go up or down, and this is our response variable. We construct a binary classification problem instead of a regression problem, as existing literature suggests that the former performs better in predicting financial market data. \cite{enke}\cite{leung}. Concretely, we specify whether the market value of a stock at a given timestep (day) will increase or decrease.

\begin{figure}[H]
  \caption{A sample of the response variable for a given time period.
  This response variable is the stock value change in \$ for the next day. Each individual timestep refers to one day.}
  \centering
    \includegraphics[width=0.5\textwidth]{dl_response_variable_.png}
\end{figure}

\section{Feature Selection}

Feature selection, the process of removing or regularizing irrelevant and/or redundant features, plays an important role in compressing the data dimension as well as improving learning accuracy, reducing learning time, and simplifying the learning results \cite{cai}. Various approaches exist and have been applied in many fields. Particularly with regard to stock market prediction, existing feature selection and feature engineering approaches included: 1) the calculation of technical attributes, such as moving averages, volatility, on balance volume, momentum, relative strength index \cite{vanstone}; 2) the application of filters and wrappers \cite{lee, lin}; as well as 3) hybrid methods \cite{tsai,lee2}.

For instance, \cite{Dai et al.} test a feature selection method termed NLICA (Nonlinear Independent Components Analysis), which assumes that measured variables can be expressed as nonlinear combinations of an arbitrary number of latent source variables. In combination with a neural network, the method outperforms both LICA and PCA feature engineering models in forecasting the Nikkei 225 as well as the Shanghai B-Share stock index.

\cite{Lendasse et al.} experiment with an approach which combines PCA and CCA with a Radial Basis Function Neural Network for time series prediction. Essentially, the paradigm used in their study encompasses the usage of a maximum number of features which are then compressed to limited state vectors and fed into the neural network.

\cite{Kumar et al.} compare different preprocessing techniques applied to neural networks and proximal support vector machines. While a random forest model outperformed its peer group significantly, other methods used (including linear correlation, rank correlation, and regression relief) achieve similar performances for each of the 12 stock indices the systems are tested on.

\cite{Lee} presents different feature selection methods applied to neural networks and support vector machines applied to predicting the NASDAQ index. These methods include correlation-based feature selection, information gain, symmetrical uncertainty, and a hybrid technique termed F-score and Supported Sequential Forward Search (F\_SSFS). F\_SSFS first filters features according to their F-Score ranks and consecutively runs a feature stacking algorithm which conducts systematic SVM trials for different feature combinations until a predefined number of features is reached or the error metric stops converging, reducing the training set based on support vector allocation at every step.

\begin{figure}[H]
  \caption{Change in stock price over a 5-day period (sampled time interval). Each individual timestep refers to one day.}
  \centering
    \includegraphics[width=0.5\textwidth]{dl_abb_5day_.png}
\end{figure}

In this study, a novel method to assess the importance of individual features in predicting the target variable (as shown in Fig. 1) is assessed.
Specifically, we look at embeddings as these can intuitively be understood as a feature selection method \cite{EmbddingFeatureSelection}, supporting the neural network in understanding 1. the relation between different features, and 2. the absolute importance of each individual features.

[TODO: Talk about how this can be seen as a latent variable, and we are maximizing a DAG-structural dependency using the latent space somehow.]

\section{Stock Embeddings}

Major research has been conducted in the field of word-embeddings [CITE], document-embeddings [CITE], and shared latent-spaces for other data-objects such as images, hand-poses [CITE], etc.
In machine learning for finance, little work has been done in identifying and analysing latent spaces amongst different investment objects, such as stocks.
Our idea of creating "stock-embeddings" stems from the fact, that little success has been achieved by just using news stories, headlines, and natural language in general, as these are heavily noisy data-sources, and stock-movement signals are often not well-understood by humans themselves.
However, due to the progress in the field of GANs and VAEs [CITE], we can use findings in the field of latent-spaces to arrive at learned embedding spaces, which do not require noisy natural language, but are coupled to the task of stock price-movement prediction.

Intuitively, the stock embeddings should allow the neural network to disentangle relations between different stocks.
Other papers \cite{OTHER} mostly focus on applying price-predictions for a general dataset of ticker items.
There is little existing literature in the field of time series analysis for stock price prediction based on cohorts.
We aim to use the stock embeddings for a multitude of reasons:

\begin{enumerate}
    \item Allow the neural to exploit learned information between stocks w.r.t. similarities and differences.
    \item Find cross-correlation between stocks. Stocks commonly exhibit the phenomenon that one stock's movement serves as an indication that another stock will move in an associated direction.
\end{enumerate}

In the neural network architecture, we use both the individual time-series as well as the stock symbol embedding matrix as inputs.
We choose the dimensions of the embedding matrix to be $(\text{"number of individual stocks"}, \text{"embedding dimensions"})$. We use 10-dimensional embeddings for all experiments as this dimensionality yields good results for the baseline neural network, and analyse the effect on performance as we perturbate the embedding dimensionality.
We discuss further results in Section IV.


\section{Model}

\subsection{Baseline}

The baseline model was taken from \cite{KaggleBaselineModel} and appropriately adapted for the present application.
It is a fully connected, non-sequential autoregressive neural network that includes an embedding layer \cite{word2vec}. The activation units are \textit{relu's} after each connected layer while a \textit{logistic sigmoid} function is applied in the output layer.
We use a single linear embedding layer for interpretability purposes, and because we want to extract the most obvious correlations between any two stocks.



\subsection{Proposed}
Our proposed neural network follows the aforementioned simple baseline network.
The following table describes the architecture of the network we use.
We use a non-sequential and autoregressive model, and will also elaborate on how the embeddings are used.
We use Keras  \cite{chollet2015keras}  to implement the model. As such, our terminology will consist of Keras modules to describe the layers we use.

The model has two inputs, one numerical input (numerical features such as 1-day-difference, 2-day-difference etc. as defined in section II), and a label input (i.e., stock symbols that are used as part of the embedding). These are split into two different streams.
Stream 1 and it's underlying fully connected neural network for the numerical items, as well as stream 2 and it's underlying fully connected neural network for the embedded items.
We pair stream 1 and stream 2 by associating each time series with the respective stock symbol. In this case, the stock symbol is passed through the embedding, resulting in a 10-dimensional embedding vector which is concatenated to the output of the time series stream.


\begin{table}[htbp]
\caption{Model overview. The neural network consists of two streams, whose outputs are concetenated and then used for binary predictions. The dimensionality of the embedding space is subject to experimtation.}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Layer No. & Type & Latent Dimensionality \\
\hline
1.1 & Input (Numerical) & - \\\hline
1.2 & Batch Normalization & - \\\hline
1.3 & Fully Connected & 128 \\\hline
1.4 & Fully Connected & 64 \\\hline

2.1 & Input (Stock Label for Embedding) & - \\\hline
2.2 & Embedding & 10* \\\hline
2.3 & Fully Connected & 32 \\\hline

3 & Concatenate (Output of 1.4 and 2.3) & - \\\hline
4 & Fully Connected & 64 \\\hline
5 & Fully Connected & 1 \\\hline

\multicolumn{3}{l}{}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

We use \textit{relu} activation units after each fully connected layer, except for the output layer for which a \textit{sigmoid} function was used.
The last layer predicts the probability that the stock will exhibit a positive return on the following day.


\section{Results \& Discussion}

\subsection{Features}
Table III presents the results from the application of the proposed feature selection algorithm for a given stock.

The application of this method does not have to be limited to the same stock but can be also be generalized to multiple stocks. It can also be applied to a wide range of other features such as interest rates changes or commodity price changes. The method is useful in financial markets because of its weak assumptions. In particular, no distribution assumptions are made and no linearity assumptions are made. Any such assumptions have repeatedly proven to be false in financial markets.
A good feature selection algorithm takes a burden of any downstream prediction algorithm.
However, in the case of stocks, hand-crafted, or natural language based features are the norm [CITE], due to the ill-understood nature of stock-movements.

\subsection{Stock Embeddings}
Given visual analysis using t-SNE, the stock embeddings seem to have learned correlations among different stocks.

\begin{figure*}
  \caption{t-SNE visualization of the embeddings. The visualized embeddings are 50 dimensional vector representations of each stock.}
  \centering
    \includegraphics[width=1\textwidth]{Embedding_Symbols.png}
\end{figure*}

One can clearly identify different clusters in Figure 4.
Additionally, we can demonstrate how different stocks are associated with each other by randomly picking stock symbols from within the embeddings.
We use the distribution of cosine-distances as a proxy to how meaningful the correlations are.
The distribution of cosine-distance values lies between $0.0$ and $1.79$, with a mean of $0.977$, implying a well-spread and thus well-enough disentangled distribution.
This disentangled representation can then be used to infer up- or down-movements based on similar performing stocks.
In Fig. 6. A. the cosine distance between the two stocks is $0.200$ (mean of cosine distribution is $0.977$), implying that both stocks are in the same cluster. "PRK" is amongst the 10 closest stocks to "DSGX" in terms of cosine-similarity.
Similarly, the cosine distance between the two stocks "IDN" and "EMMS" is $0.128$ (mean of cosine distribution is $0.977$), implying that both stocks are in the same cluster. "EMMS" is amongst the 10 closest stocks to "IDN" in terms of cosine-similarity.

\begin{figure}[H]
  \caption{One example for the neighborhood of the embeddings, when BlackRock is chosen as the asset under comparison}
  \centering
    \includegraphics[width=0.5\textwidth]{Blackrock.png}
\end{figure}

One can also clearly identify clustering tendencies for securities with similar price performance trajectories. For instance, two stocks starting high and falling low during the testing period will more likely be grouped together in the latent space created by the embeddings (see Fig. 6 in App. A).
From this figure, one can observe that the embeddings have learned a correlation between stocks that are decreasing in value.

\subsection{Training}

All models were trained, validated, and tested on the exact same sample set, which provides a solid ground for comparison between models.
Given the nature of the binary classification task, we used the binary cross-entropy loss-measure for training and comparison across models.
We decided to not use the validation loss, but to use accuracy measures (given in \%) as a comparative metric as this gives more intuition and is directly related to the binary cross-entropy of the model.


%\begin{figure}[h]
%\includegraphics[width=8cm]{nn.svg}
%\end{figure}
\begin{table}[H]
\caption{Reference models (non-neural network models) overview}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Model} & Train Accuracy & Test Accuracy \\
\hline
Random        & 50.16 & 50.19 \\\hline
Decision Tree & 53.76 & 53.46  \\\hline
XGBoost      & 53.93 & 53.56 \\\hline
\multicolumn{3}{l}{}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

As hyperparameters, we set "min samples leaf = 5000" for the decision tree, and kept the default hyperparameters for all the other models.
As one can see, the xgboost (which employs a linear gradient algorithm) has the highest performance.


%\begin{figure}[h]
%\includegraphics[width=8cm]{nn.svg}
%\end{figure}
\begin{table}[H]
\caption{Accuracy of different neural network architectures with different configurations.}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Embedding} & Epochs & Train & Test \\
& & Accuracy & Accuracy\\
\hline
Yes & 3 & 53.55 & 53.48 \\\hline
No & 20 &  53.44 & 53.49 \\\hline
Yes & 20 & 54.22 & \textbf{54.03} \\\hline
\multicolumn{3}{l}{}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

It can be clearly seen that the neural network model with the stock embeddings performs . The improvement due to the embeddings are significant (compared to non-embeddings), as this is multiple hundreds of percentage points away from the XGBoost model, as well as the non-embeddings model (on a test set with 1 million samples). This implies that the learned results are significant, and not due to random fluctuations.

Finally, we observe results when altering the embedding dimensions.
We pick the values 10, 50 and 300 because these are respectively common values when data is limited (10 and 50 dimensions), or for similar tasks in the domain of natural language processing (300 dimensions) \cite{GLOVE}.

%\begin{figure}[h]
%\includegraphics[width=8cm]{nn.svg}
%\end{figure}
\begin{table}[H]
\caption{Model accuracy relative to the embedding dimensions}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Embedding dimension} & Train Accuracy & Test Accuracy \\
\hline
5 & 53.94 & \textbf{54.89} \\\hline
10 & 54.22 & 54.03 \\\hline
50 & 54.44 & 54.48  \\\hline
300 & 53.55 & 54.77 \\\hline
\multicolumn{3}{l}{}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

The dependence between the embedding dimensions and the accuracy confirms once more that there is a significant information leverage from increasing dimensionality until some exhaustive dimension after which the additional benefits are minor.

As such, we analyse the confusion matrix to analyse in what direction our predictions are skewed to.

\begin{table}[H]
\caption{Normalized confusion matrix. All values are expressed as a percentage value.}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
 & Predicted Sell & Predicted Buy \\
\hline
Real Sell & 7.61 \% & 38.46 \% \\\hline
Real Buy & 6.64 \% & 47.28 \% \\\hline
\multicolumn{3}{l}{}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

Because in financial markets, a decision to buy is as important as a decision to sell (assuming a balanced dataset), we use the f1-score to arrive at a score which balances the trade-off between precision and recall, using the following formula:

$$
F1 = 2 \cdot \frac{precision\cdot recall}{precision+ recall}
$$

We arrive at the metrics of precision, recall and the f1-score by calculating the predictions on the test-set.

\begin{table}[H]
\caption{Normalized confusion matrix. All values are expressed as a percentage value.}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
 Metric & Score \\
\hline
Precision & 55.14 \% \\\hline
Recall & 87.68 \% \\\hline
F-1 &  67.71 \% \\\hline
\multicolumn{2}{l}{}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

- We compare with a model which does a "buy-only" policy. A buy-only policy has a precision of 53.9, which is considerably worse than our measured predictions.

- f1 score is relatively high, this indicates
- strong skew to buying
- why is there a strong skew to buying?
- possibly because stocks usually go up, in times of "non-inflation", and inflation has heavy downturns (i.e. usually skewed to "ups").
- @Lukas, any ideas here?


\begin{figure}[H]
  \caption{Distribution overlap between predicted and actual daily returns in USD. A strong skew towards positive values can be seen for predicted items.}
  \centering
    \includegraphics[width=0.5\textwidth]{histogram_distribution.png}
\end{figure}

- Precision is low due to heavily predicting responses as "positive".

\begin{figure}[H]
  \caption{MAE histogram. Number of items with a given mean absolute error value. }
  \centering
    \includegraphics[width=0.5\textwidth]{histogram_mae.png}
\end{figure}


Although achieving an f1-score of $67.71 \%$ is considered a non-random variable, indicating that the model has learned valuable signals for stock price prediction, there is a heavy skew to buying stocks rather to not buying them.
This is possibly due to the nature of stocks, whose movement is contrary to inflation, and whose value compounds over time.

The accuracy is just marginally above completely random predictions, through which the curve for our model isstands close to a straight line.

\begin{figure}[H]
  \caption{ROC curve}
  \centering
    \includegraphics[width=0.5\textwidth]{roc.png}
\end{figure}


\subsection{Qualitative evaluation of embeddings}

- Embeddings seem to capture relationships between stocks which have a general absolute number of positive days
- Embeddings do seem to capture to some extent different sectors (i.e. tech-companies Amazon, Baidu are very close in the cluster)
- This can be possibly seen as a good predictor (apart from recession, and bubbles) of whether a stock is going to go up or down in a long-term perspective.

\begin{figure}[htpb]
  \caption{Stocks "DSGX" and "PRK": Stock price in USD between 2000 and 2019. There is a general positive trend w.r.t. time.}
  \centering
    \includegraphics[width=0.5\textwidth]{DSGX_2.png}
    \includegraphics[width=0.5\textwidth]{PRK_2.png}
\end{figure}

\section{Conclusion}

Deep learning architectures are increasingly gaining relevance in the fields of sequence prediction and computer vision tasks.
However, the use of deep learning models within the stock market is rather scarce compared to traditional machine learning algorithms and traditional stochastic prediction models.
In this paper we demonstrated that deep neural network models represent highly competitive algorithms for stock price prediction.
Firstly, we compared traditional machine learning algorithms with a neural network baseline, showing that the neural network is indeed capable of achieving similar accuracy levels.
Second, we introduced a novel neural architecture model through the use of stock embeddings (similar to word embeddings), which achieves state-of-the-art accuracy in the task of stock price prediction.

Lastly, we also showed that the stock embeddings can intuitively be can be used as a feature selection method in stock market prediction. To the best of our knowledge, the use of this method has not been investigated before.
These contributions pave the way for future research in the direction of using embeddings in domains different than natural language processing.

\begin{thebibliography}{00}
\bibitem{chollet2015keras}{Chollet, F. & et al., "Keras", \textcolor{blue}{\url{https://keras.io}} (2015),
}

\bibitem{tsai}{Tsai, C. & Yu-Chieh H. (2010) "Combining multiple feature selection methods for stock prediction: Union, intersection, and multi-intersection approaches." Decision Support Systems 50.1: 258-269.}

\bibitem{lee2}{Lee, Ming-Chi. (2009) "Using support vector machine with a hybrid feature selection method to the stock trend prediction." Expert Systems with Applications 36.8 : 10896-10904.}

\bibitem{cai}{Cai, J., et al. (2018) "Feature selection in machine learning: A new perspective." Neurocomputing 300 : 70-79.}

\bibitem{lee}{Lee, Ming-Chi. (2009) "Using support vector machine with a hybrid feature selection method to the stock trend prediction." Expert Systems with Applications 36.8 : 10896-10904.}
\bibitem{vanstone}{Vanstone B., & Gavin F (2009). "An empirical methodology for developing stockmarket trading systems using artificial neural networks." Expert systems with applications 36.3 : 6668-6680.}

\bibitem{Dai et al.} Dai, W., Wu, J. Y., & Lu, C. J. (2012). Combining nonlinear independent component analysis and neural network for the prediction of Asian stock market indexes. Expert systems with applications, 39(4), 4444-4452.

\bibitem{Kumar et al.} Kumar, D., Meghwani, S. S., & Thakur, M. (2016). Proximal support vector machine based hybrid prediction models for trend forecasting in financial markets. Journal of Computational Science, 17, 1-13.

\bibitem{Lee} Lee, M. C. (2009). Using support vector machine with a hybrid feature selection method to the stock trend prediction. Expert Systems with Applications, 36(8), 10896-10904.

\bibitem{Lendasse et al.} Lendasse, A., de Bodt, E., Wertz, V., & Verleysen, M. (2000). Non-linear financial time series forecasting-Application to the Bel 20 stock market index. European Journal of Economic and Social Systems, 14(1), 81-91.

\bibitem{lin}{Lin, Fengyi, et al. (2014) "Novel feature selection methods to financial distress prediction." Expert Systems with Applications 41.5: 2472-2483.}

\bibitem{EmbddingFeatureSelection}{Hou, Chenping, et al (2014). Joint Embedding Learning and Sparse Regression:
A Framework for Unsupervised Feature Selection}

\bibitem{KaggleBaselineModel}{Anonymous (2018), Market Data NN Baseline,  https://www.kaggle.com/christofhenkel/market-data-nn-baseline, last viewed 14. April 2019}

\bibitem{kuremoto}{Kuremoto, Takashi, et al. (2014) "Time series forecasting using a deep belief network with restricted Boltzmann machines." Neurocomputing 137 : 47-56.}

\bibitem{yoshihara}{Yoshihara, Akira, et al. (2014) "Predicting stock market trends by recurrent deep neural networks." Pacific rim international conference on artificial intelligence. Springer, Cham.}
\bibitem{ding}{Ding, Xiao, et al. "Deep learning for event-driven stock prediction." Ijcai. 2015.}

\bibitem{OTHER}{Xiao Ding, Yue Zhang, Ting Liu, Junwen Duan (2015), Deep Learning for Event-Driven Stock Prediction}

\bibitem{cavalcante}{Cavalcante, Rodolfo C., et al. (2016) "Computational intelligence and financial markets: A survey and future directions." Expert Systems with Applications 55 : 194-211.}

\bibitem{lasfer}{Lasfer, Assia, El-Baz H., & Zualkernan I. (2013) "Neural Network design parameters for forecasting financial time series." Modeling, Simulation and Applied Optimization (ICMSAO), 2013 5th International Conference on. IEEE.}

\bibitem{dhar}{Dhar, Satyajit, Tuhin Mukherjee, and Arnab Kumar Ghoshal. (2010) "Performance evaluation of Neural Network approach in financial prediction: Evidence from Indian Market." Communication and Computational Intelligence (INCOCCI), 2010 International Conference on. IEEE.}

\bibitem{bao}{Bao, Yukun, et al. (2011) "A comparative study of multi-step-ahead prediction for crude oil price with support vector regression." Computational Sciences and Optimization (CSO), 2011 Fourth International Joint Conference on. IEEE.}

\bibitem{xie}{Xie, Guo-qiang. (2011) "The optimization of share price prediction model based on support vector machine." Control, Automation and Systems Engineering (CASE), 2011 International Conference on. IEEE.}

\bibitem{zhu}{Zhu, Bangzhu, and Yiming Wei (2013). "Carbon price forecasting with a novel hybrid ARIMA and least squares support vector machines methodology." Omega 41.3 : 517-524.}

\bibitem{patel}{Patel, Jigar, et al. (2015) "Predicting stock market index using fusion of machine learning techniques." Expert Systems with Applications 42.4: 2162-2172.}

\bibitem{martinez}{Martinez, Leonardo C., et al. (2009) "From an artificial neural network to a stock market day-trading system: A case study on the bm&f bovespa." Neural Networks, 2009. IJCNN 2009. International Joint Conference on. IEEE, 2009.}

\bibitem{enke}Enke, D. and Thawornwong, S. (2005). The use of data mining and neural networks for forecasting
stock market returns. Expert Systems with Applications, 29(4):927–940.

\bibitem{leung}Leung, M. T., Daouk, H., and Chen, A.-S. (2000). Forecasting stock indices: A comparison of classification and level estimation models. International Journal of Forecasting, 16(2):173–190.

\bibitem{lippiello}Lippiello E., de Arcangelis L. & Godano C. (2008) Influence of Time and Space Correlations on Earthquake Magnitude.
Physical Review Letters, published 22 January 2008

\bibitem{mendoza}Mendoza M., Kaydul A., de Arcangelis L., Andrade Jr J.S. and Herrmann H.J. (2014), Modelling the influence of photospheric turbulence on solar flare statistics, Nature Communications, Published 23 Sep 2014

\bibitem{GLOVE}{Pennington J., Socher R., and Manning C.D. (2014). GloVe: Global Vectors for Word Representation}

\bibitem{Gopikrishnan} Gopikrishnan, Parameswaran, et al. (1999) Scaling of the distribution of fluctuations of financial market indices. Physical Review E 60.5 (1999): 5305.

\bibitem{word2vec} Mikolov T., et al. (2013) Distributed Representations of Words and Phrases and their Compositionality. NIPS Proceedings 2013.

\end{thebibliography}
\clearpage
\section{Appendum A}

One can observe from the following graphs that the embeddings were able to capture stock items that would also show a negative (IDN, EMMS) trend (similar to Fig. 6, which captures positive trends between stocks) .

\begin{figure}[htpb]
  \caption{Stocks "IDN" and "EMMS": Stock price in USD between 2000 and 2019. There is a general negative trend w.r.t. time.}
  \centering
    \includegraphics[width=0.5\textwidth]{IDN_1.png}
    \includegraphics[width=0.5\textwidth]{EMMS_1.png}
\end{figure}

\makeatletter
\setlength{\@fptop}{63pt}
\makeatother
\clearpage
\makeatletter
\setlength{\@fptop}{0pt}
\makeatother
\begin{figure}[!htpb]
  \caption{t-SNE visualization of the embeddings. The visualized embeddings are 50 dimensional vector representations of each stock.}
  \centering
    \includegraphics[width=1.0\textwidth]{Embedding_Dots.png}
\end{figure}

\end{document}
